{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QG vs KK 10-03-19.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDWIf_Wh3DLG"
      },
      "source": [
        "!pip install pygooglenews --upgrade\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbfZOgna3EmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5859e436-8ac3-44a7-8b10-2a4600f0fd4a"
      },
      "source": [
        "from pygooglenews import GoogleNews\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "import math\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.append('-')\n",
        "stop_words.append('google')\n",
        "stop_words.append('news')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuHq88vk7yfW",
        "outputId": "e38d1aa1-b03e-4e04-a0ba-909118844a96"
      },
      "source": [
        "gn = GoogleNews()\n",
        "\n",
        "s = gn.search('intitle:quetta gladiators vs karachi kings', from_='2019-03-10', to_='2019-03-12')\n",
        "\n",
        "print(s['feed'].title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"intitle:quetta gladiators vs karachi kings after:2019-03-10 before:2019-03-12\" - Google News\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPKfFELiWNFR"
      },
      "source": [
        "def words_for_images(title):\n",
        "  new_lst = []\n",
        "  required_img = []\n",
        "\n",
        "  lst = title\n",
        "  lst = lst.lower()\n",
        "  lst = lst.split()\n",
        "  #print(lst)\n",
        "\n",
        "  #lst.replace('\\\"','')\n",
        "  for word in lst:\n",
        "    word = word.replace('\\\"','')\n",
        "    word = word.replace('intitle:','')\n",
        "    new_lst.append(word)\n",
        "  \n",
        "  for word in new_lst:\n",
        "    if 'after' in word:\n",
        "      new_lst.remove(word)\n",
        "\n",
        "  for word in new_lst:  \n",
        "    if 'before' in word:\n",
        "      new_lst.remove(word)\n",
        "\n",
        "  for word in new_lst:\n",
        "    if word not in stop_words:\n",
        "      required_img.append(word)\n",
        "\n",
        "  #print(required_img)\n",
        "  return required_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNbi0zviCDDH"
      },
      "source": [
        "def required_images(images, required_img):\n",
        "  images_lst = []\n",
        "  for img_name in images:\n",
        "    if any(x in str(img_name).lower() for x in required_img):\n",
        "      images_lst.append(str(img_name))\n",
        "\n",
        "  return images_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEiug6QGC1nt"
      },
      "source": [
        "# This function will return the number from 1 - 5. 1 will be extremely negative and 5 will be extremely positive\n",
        "\n",
        "def sentiment_analysis(text):\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "  model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "  #tokens = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "  num = math.ceil(len(text)/900)\n",
        "\n",
        "  sent_score = []\n",
        "  a = 0\n",
        "  b = 900\n",
        "\n",
        "  for iter in range(num):\n",
        "    sub_text = text[a:b]\n",
        "    #sub_tokens = tokens[a:b]\n",
        "    tokens = tokenizer.encode(sub_text, return_tensors='pt')\n",
        "    result = model(tokens)\n",
        "\n",
        "    result_num = int(torch.argmax(result.logits)) + 1\n",
        "    sent_score.append(result_num)\n",
        "\n",
        "    a += 900\n",
        "    b += 900\n",
        "  #print(sent_score); exit()\n",
        "  final_result = sum(sent_score)/len(sent_score)\n",
        "  \n",
        "  return final_result\n",
        "  \n",
        "  \n",
        "  # result.logits   # To check the value of results\n",
        "\n",
        "  #return int(torch.argmax(result.logits)) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXvjHdKv_vAT",
        "outputId": "e11ebded-4692-4707-d59f-6b1d2a4ce499"
      },
      "source": [
        "\n",
        "\n",
        "total = len(s['entries'])\n",
        "\n",
        "for tot in range(total):\n",
        "\n",
        "  for a in range(len(s['entries'][tot].sub_articles)):\n",
        "    full_text = []\n",
        "    cleaned_text = []\n",
        "\n",
        "    url = s['entries'][tot].sub_articles[a]['url']\n",
        "    pub = s['entries'][tot].sub_articles[a]['publisher']\n",
        "    title = s['entries'][tot].sub_articles[a]['title']\n",
        "  \n",
        "    page = requests.get(url)\n",
        "\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    #print(soup.body.text)\n",
        "    for b in soup.body.stripped_strings:\n",
        "      full_text.append(b)\n",
        "\n",
        "    for c in full_text:\n",
        "      if len(c)<130:\n",
        "        continue\n",
        "      else:\n",
        "        cleaned_text.append(c)\n",
        "\n",
        "    cleaned_text = ' '.join(cleaned_text)\n",
        "\n",
        "    print(s['entries'][tot].sub_articles[a]['title'])\n",
        "\n",
        "    images = soup.find_all('img')\n",
        "\n",
        "    doc_title = s['feed'].title\n",
        "    required_images_words = words_for_images(doc_title)\n",
        "\n",
        "    our_images = required_images(images, required_images_words)\n",
        "\n",
        "    # Calculating sentiment analysis\n",
        "    sentiment_score = sentiment_analysis(cleaned_text)\n",
        "\n",
        "    # Writing to the files\n",
        "    file1 = open(title+'.txt', 'w')\n",
        "    file1.write('url: '+url+'\\n')\n",
        "    file1.write('Publisher: '+pub + '\\n')\n",
        "    file1.write('Title: ' + title + '\\n\\n')\n",
        "\n",
        "    file1.writelines(cleaned_text + '\\n\\n')\n",
        "    file1.write('Sentiment Score: ' + str(sentiment_score) + '\\n\\n')\n",
        "    file1.write('Images links \\n')\n",
        "    file1.writelines(our_images)\n",
        "    file1.close()\n",
        "    #break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Karachi Kings snatch victory away from Quetta Gladiators in dramatic clash on home turf\n",
            "Karachi Kings beat Quetta Gladiators to reach playoffs\n",
            "Karachi Kings beat Quetta Gladiators by 1 run in PSL Karachi Stadium thriller\n",
            "PSL 2019: Quetta Gladiators coach Moin Khan envisions bright Pakistan future for Umar Akmal and Ahmed Shehzad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsYuqG7TE3W2",
        "outputId": "15291657-1a4f-41b0-8b6d-3c6ca7924653"
      },
      "source": [
        "sentiment_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGn2vzqgBMkN"
      },
      "source": [
        "###Practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KemLjId3Wfj"
      },
      "source": [
        "gn = GoogleNews(lang = 'en', country = 'PK')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NAGWYzB3avr"
      },
      "source": [
        "top_news = gn.top_news()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPzhJRyC3dem"
      },
      "source": [
        "top_news"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OhcvqsKIwcb"
      },
      "source": [
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}